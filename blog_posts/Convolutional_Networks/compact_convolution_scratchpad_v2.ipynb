{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import signal as sig\n",
    "\n",
    "# imports from custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('text', usetex=True)\n",
    "from mlrefined_libraries import convnets_library as convlib\n",
    "from mlrefined_libraries import basics_library as baslib\n",
    "from mlrefined_libraries import superlearn_library as superlearn\n",
    "from mlrefined_libraries import multilayer_perceptron_library as network_lib\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import grad as compute_grad   \n",
    "\n",
    "import autograd.numpy as np\n",
    "import numpy as npo\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "import glob\n",
    "import time\n",
    "import copy\n",
    "from datetime import datetime \n",
    "\n",
    "#this is needed to compensate for matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test transformation on face images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data, transform via original method, transform via new method, compare features to make sure everything looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def load_data(datapath):\n",
    "    # load in data\n",
    "    data = np.loadtxt(datapath,delimiter = ',')\n",
    "\n",
    "    # import data and reshape appropriately\n",
    "    X = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "    y.shape = (len(y),1)\n",
    "    \n",
    "    X_square = np.zeros((len(y),28,28))\n",
    "    for i in range(0,len(y)):\n",
    "        X_square[i,:,:] = np.reshape(X[i,:],(28,28),1)\n",
    "    \n",
    "    # pad data with ones for more compact gradient computation\n",
    "    o = np.ones((np.shape(X)[0],1))\n",
    "    X = np.concatenate((o,X),axis = 1)\n",
    "    X = X.T\n",
    "    \n",
    "    return X,X_square,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "datapath = '../../mlrefined_datasets/convnet_datasets/feat_face_data.csv'\n",
    "X,image_tensor, y = load_data(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_tensor = image_tensor[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load kernels\n",
    "kernels = convlib.image_viz.load_kernels()\n",
    "\n",
    "# params\n",
    "sliding_window_size = (6,6) \n",
    "stride=3\n",
    "pooling_func= 'max'\n",
    "\n",
    "# get number of images in the dataset\n",
    "num_images = np.shape(image_tensor)[0]\n",
    "        \n",
    "# a test run to find the number of features with the params above\n",
    "test = convlib.image_viz.make_feat(image_tensor[0,:,:], kernels, sliding_window_size=sliding_window_size, stride=stride)\n",
    "num_features = np.shape(test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elpased (hh:mm:ss.ms) 0:00:00.326260\n"
     ]
    }
   ],
   "source": [
    "# start timer\n",
    "startTime= datetime.now() \n",
    "\n",
    "# run old method\n",
    "feat = []\n",
    "for i in range(0,num_images):\n",
    "    # extract features\n",
    "    f = convlib.image_viz.make_feat(image_tensor[i,:,:], kernels, sliding_window_size=sliding_window_size,\n",
    "                                            stride=stride, pooling_func=pooling_func) \n",
    "    # store it\n",
    "    feat.append(f)\n",
    "    \n",
    "# convert to array\n",
    "feat = np.asarray(feat)\n",
    "\n",
    "# time for measurment\n",
    "timeElapsed=datetime.now()-startTime \n",
    "\n",
    "print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 512)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most naive image version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### parameters for naive convolution ###\n",
    "conv_function = lambda window: np.sum(kernel*window)\n",
    "\n",
    "# pooling / downsampling parameters\n",
    "pool_function = lambda window: np.max(window) \n",
    "\n",
    "# activation \n",
    "activation = lambda window: np.maximum(0,window)\n",
    "\n",
    "# pad image with appropriate number of zeros for convolution\n",
    "def pad_image(image,kernel_size):\n",
    "    odd_nums = np.array([int(2*n + 1) for n in range(100)])\n",
    "    pad_val = np.argwhere(odd_nums == kernel_size)[0][0]\n",
    "    image_padded = np.zeros((np.shape(image) + 2*pad_val))\n",
    "    image_padded[pad_val:-pad_val,pad_val:-pad_val] = image\n",
    "    return image_padded   \n",
    "\n",
    "def sliding_window(image,window_size,stride,func):\n",
    "    # grab image size, set container for results\n",
    "    image_size = np.shape(image)[0]\n",
    "    results = []\n",
    "    \n",
    "    # slide window over input image with given window size / stride and function\n",
    "    for i in np.arange(0, image_size - window_size + 1, stride):\n",
    "        for j in np.arange(0, image_size - window_size + 1, stride):\n",
    "            # now we have a window from our image, and use the desired 'func' to process it\n",
    "            window = image[i:i+window_size,j:j+window_size]\n",
    "            \n",
    "            # process using input func\n",
    "            processed_window = func(window)\n",
    "            results.append(processed_window)\n",
    "    \n",
    "    # array-afy results\n",
    "    results = np.array(results)\n",
    "    \n",
    "    # return results in numpy array format\n",
    "    return results\n",
    "\n",
    "def make_feature_map(image,kernel):\n",
    "    # pad image appropriately\n",
    "    kernel_size = kernel.shape[0]\n",
    "    padded_image = pad_image(image,kernel_size)\n",
    "    \n",
    "    # create feature map via convolution --> returns flattened convolution calculations\n",
    "    conv_stride = 1\n",
    "    feature_map = sliding_window(padded_image,kernel_size,conv_stride,conv_function)\n",
    "    \n",
    "    # re-shape convolution output ---> to square of same size as original input\n",
    "    new_size = int(np.size(feature_map)**(0.5))\n",
    "    feature_map = np.reshape(feature_map,(new_size,new_size))\n",
    "    \n",
    "    # shove feature map through nonlinearity\n",
    "    feature_map = activation(feature_map)\n",
    "\n",
    "    # pool feature map --- i.e., downsample it\n",
    "    pool_window_size = 6\n",
    "    pool_stride = 3\n",
    "    downsampled_feature_map = sliding_window(feature_map,pool_window_size,pool_stride,pool_function)\n",
    " \n",
    "    # return downsampled feature map --> flattened\n",
    "    return downsampled_feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernels = np.array([\n",
    "       [[-1, -1, -1],\n",
    "        [ 0,  0,  0],\n",
    "        [ 1,  1,  1]],\n",
    "\n",
    "       [[-1, -1,  0],\n",
    "        [-1,  0,  1],\n",
    "        [ 0,  1,  1]],\n",
    "    \n",
    "        [[-1,  0,  1],\n",
    "        [-1,  0,  1],\n",
    "        [-1,  0,  1]],\n",
    "\n",
    "       [[ 0,  1,  1],\n",
    "        [-1,  0,  1],\n",
    "        [-1, -1,  0]],\n",
    "\n",
    "       [[ 1,  0, -1],\n",
    "        [ 1,  0, -1],\n",
    "        [ 1,  0, -1]],\n",
    "\n",
    "       [[ 0, -1, -1],\n",
    "        [ 1,  0, -1],\n",
    "        [ 1,  1,  0]],\n",
    "\n",
    "       [[ 1,  1,  1],\n",
    "        [ 0,  0,  0],\n",
    "        [-1, -1, -1]],\n",
    "\n",
    "       [[ 1,  1,  0],\n",
    "        [ 1,  0, -1],\n",
    "        [ 0, -1, -1]]])\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel = kernels[0]\n",
    "all_feature_maps = []\n",
    "for image in image_tensor:\n",
    "    current_feat_maps = []\n",
    "    for kernel in kernels:\n",
    "        # compute feature map for current image using current convolution kernel\n",
    "        feat_map = make_feature_map(image,kernel)\n",
    "\n",
    "        # store feature maps of current kernel\n",
    "        current_feat_maps.append(feat_map)\n",
    "    \n",
    "    # append all feature maps from current kernel to running list\n",
    "    all_feature_maps.append(current_feat_maps)\n",
    "\n",
    "# convert to array and re-shape properly\n",
    "all_feature_maps = np.array(all_feature_maps)\n",
    "all_feature_maps = np.reshape(all_feature_maps,(np.shape(all_feature_maps)[0],np.shape(all_feature_maps)[1]*np.shape(all_feature_maps)[2]),order = 'F')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
