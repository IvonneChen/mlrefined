{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code cell will not be shown in the HTML version of this notebook\n",
    "# import custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from mlrefined_libraries import superlearn_library as superlearn\n",
    "from mlrefined_libraries import math_optimization_library as optlib\n",
    "datapath = '../../mlrefined_datasets/superlearn_datasets/'\n",
    "\n",
    "# demos for this notebook\n",
    "classif_plotter = superlearn.lin_classification_demos\n",
    "optimizers = optlib.optimizers\n",
    "classification_plotter = superlearn.classification_static_plotter.Visualizer();\n",
    "feature_scaling_tools = superlearn.feature_scaling_tools\n",
    "static_plotter = optlib.static_plotter.Visualizer()\n",
    "\n",
    "cost_lib = superlearn.cost_functions\n",
    "\n",
    "# import autograd functionality to bulid function's properly for optimizers\n",
    "import autograd.numpy as np\n",
    "\n",
    "# import timer\n",
    "from datetime import datetime \n",
    "\n",
    "# this is needed to compensate for %matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a class per \n",
    "\n",
    "- input normalization / loading\n",
    "\n",
    "- model\n",
    "\n",
    "- cost function\n",
    "\n",
    "- optimizer\n",
    "\n",
    "- various plotting tools for visualization / debugging\n",
    "\n",
    "Each needs to be made to intake new models we will develop in the chapter to follow.  Lets start with normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class input_normalizer:\n",
    "    '''\n",
    "    A class that wraps up the various input normalization schemes\n",
    "    we have seen including\n",
    "    - mean centering / std normalization\n",
    "    - PCA sphereing\n",
    "    - ZCA sphereing\n",
    "    \n",
    "    For each scheme you put in input features, and the following is returned\n",
    "    - normalizer: the normalization scheme of your choice, returned as a function that \n",
    "    you can then use for future test points\n",
    "    - inverse_normalizer: inverse normalization function for reversing the chosen \n",
    "    normalization\n",
    "    \n",
    "    You can then normalize the input x of a dataset using the desired normalization scheme\n",
    "    by \n",
    "    \n",
    "    x_normalized = normalizer(x)\n",
    "    \n",
    "    and then return the data to its original form as\n",
    "    \n",
    "    x_orig = inverse_normalizer(x_normalized)\n",
    "    '''\n",
    "    \n",
    "    def create_functions(self,x,scheme,**kwargs):\n",
    "        normalizer = 0\n",
    "        inverse_normalizer = 0\n",
    "        \n",
    "        # standard normalization - for each feature subtract mean, divide by standard deviation \n",
    "        if scheme == 'standard':\n",
    "            normalizer, inverse_normalizer = self.standard(x)\n",
    "        \n",
    "        # PCA-sphereing - use PCA to normailze input features\n",
    "        if scheme =='PCA-sphereing':\n",
    "            normalizer, inverse_normalizer = self.PCA_sphere(x,**kwargs)\n",
    "        \n",
    "        return normalizer, inverse_normalizer\n",
    "\n",
    "    # standard normalizer - subtract mean, divide by standard deviation - for each input feature\n",
    "    def standard_normalizer(self,x):\n",
    "        # compute mean / std of each input feature\n",
    "        x_means = np.mean(x,axis = 1)[:,np.newaxis]\n",
    "        x_stds = np.std(x,axis = 1)[:,np.newaxis]    \n",
    "\n",
    "        # create normalizer and input normalizer functions based on mean / std\n",
    "        normalizer = lambda data: (data - x_means)/x_stds\n",
    "        \n",
    "        # create inverse normalizer function \n",
    "        inverse_normalizer = lambda data: data*x_stds + x_means\n",
    "        \n",
    "        return normalizer, inverse_normalizer\n",
    "        \n",
    "    ##### PCA-sphereing functions ####\n",
    "    # PCA-sphereing - use PCA to normalize input features\n",
    "    def PCA_sphereing(self,x,**kwargs):\n",
    "        # standard normalize the input data\n",
    "        standard_normalizer, inv_standard_normalizer = self.standard(x)\n",
    "        \n",
    "        # compute pca transform and inverse transform for sphereing\n",
    "        D,V = self.PCA(x_standard,**kwargs)\n",
    "        D1 = np.array([d**(0.5) for d in D])\n",
    "        D2 = np.array([1/d**(0.5) for d in D])\n",
    "        D1_full = np.diag(D1)\n",
    "        D2_full = np.diag(D2)\n",
    "        M = np.dot(D2_full,V.T)\n",
    "        M_inv = np.dot(V,D1_full)\n",
    "        \n",
    "        # make normalizer and inverse normalizer\n",
    "        normalizer = lambda data: np.dot(M,standard_normalizer(data))\n",
    "        inverse_normalizer = lambda data: np.dot(M_inv,inv_standard_normalizer(data))\n",
    "\n",
    "        return normalizer, inverse_normalizer\n",
    "    \n",
    "    # compute eigendecomposition of data covariance matrix\n",
    "    def PCA(self,x,**kwargs):\n",
    "        '''\n",
    "        A function for producing the full PCA transformation on an input dataset.  \n",
    "        '''\n",
    "        lam = 10**(-7)\n",
    "        if 'lam' in kwargs:\n",
    "            lam = kwargs['lam']\n",
    "            \n",
    "        # create the correlation matrix\n",
    "        P = float(x.shape[1])\n",
    "        Cov = 1/P*np.dot(x,x.T) + lam*np.eye(x.shape[0])\n",
    "\n",
    "        # use numpy function to compute eigenvalues / vectors of correlation matrix\n",
    "        D,V = np.linalg.eigh(Cov)\n",
    "        return D,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datapath = '../../mlrefined_datasets/superlearn_datasets/'\n",
    "data = np.loadtxt(datapath + '2d_classification_data_v1.csv')\n",
    "\n",
    "# get input/output pairs\n",
    "x = data[:,:-1:].T\n",
    "y = data[:,-1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalization test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in dataset\n",
    "data = np.loadtxt(datapath + 'breast_cancer_data.csv',delimiter = ',')\n",
    "\n",
    "# get input/output pairs\n",
    "x = data[:,:-1:].T\n",
    "y = data[:,-1:] \n",
    "\n",
    "# normalize input\n",
    "a = input_normalizer()\n",
    "scheme = 'standard'\n",
    "normalizer, inverse_normalizer = a.create_functions(x,scheme)\n",
    "x_normalized = normalizer(x)\n",
    "\n",
    "# tack a 1 onto the top of each input point\n",
    "o = np.ones((1,np.shape(x_orig)[1]));\n",
    "x = np.concatenate((o,x_orig),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fully evaluate our network features using the tensor of weights in omega_inner\n",
    "def compute_features(x, omega_inner):\n",
    "    o = np.ones((np.shape(x)[0],1))\n",
    "    a_padded = np.concatenate((o,x),axis = 1)\n",
    "    \n",
    "    # loop through each layer matrix\n",
    "    for W in omega_inner:\n",
    "        # output of layer activation\n",
    "        a = activation(np.dot(a_padded,W))\n",
    "                \n",
    "        #  pad with ones (to compactly take care of bias) for next layer computation\n",
    "        o = np.ones((np.shape(a)[0],1))\n",
    "        a_padded = np.concatenate((o,a),axis = 1)\n",
    "        \n",
    "    return a_padded\n",
    "\n",
    "# our predict function \n",
    "def predict(x,omega):     \n",
    "    # compute network features - here omega[0] contains the entire tensor of internal weights\n",
    "    f = compute_features(x,omega[0])\n",
    "    \n",
    "    # compute linear model compactly via inner product - here omega[1] contains only those weights in the final linear combination of network features\n",
    "    vals = np.dot(f,omega[1])\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One versus all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code cell will not be shown in the HTML version of this notebook\n",
    "# import custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from mlrefined_libraries import superlearn_library as superlearn\n",
    "from mlrefined_libraries import math_optimization_library as optlib\n",
    "datapath = '../../mlrefined_datasets/superlearn_datasets/'\n",
    "\n",
    "# demos for this notebook\n",
    "optimizers = optlib.optimizers\n",
    "cost_lib = superlearn.cost_functions\n",
    "normalizers = superlearn.normalizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell will not be shown in the HTML version of this notebook\n",
    "# import custom library\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from mlrefined_libraries import superlearn_library as superlearn\n",
    "from mlrefined_libraries import math_optimization_library as optlib\n",
    "datapath = '../../mlrefined_datasets/superlearn_datasets/'\n",
    "\n",
    "# demos for this notebook\n",
    "optimizers = optlib.optimizers\n",
    "cost_lib = superlearn.cost_functions\n",
    "normalizers = superlearn.normalizers \n",
    "\n",
    "### compare grad descent runs - given cost to counting cost ###\n",
    "def one_versus_all(x,y,**kwargs):\n",
    "    # get and run optimizer to solve two-class problem\n",
    "    N = np.shape(x)[0]\n",
    "    C = np.size(np.unique(y))\n",
    "    max_its = 100; alpha_choice = 1; cost_name = 'softmax'; w = 0.1*np.random.randn(N+1,1);\n",
    "    \n",
    "    # switches for user choices\n",
    "    if 'max_its' in kwargs:\n",
    "        max_its = kwargs['max_its']\n",
    "    if 'alpha_choice' in kwargs:\n",
    "        alpha_choice = kwargs['alpha_choice']\n",
    "    if 'cost_name' in kwargs:\n",
    "        cost_name = kwargs['cost_name']\n",
    "    if 'w' in kwargs:\n",
    "        w = kwargs['w']\n",
    "    \n",
    "    # loop over subproblems and solve\n",
    "    weight_histories = []\n",
    "    for c in range(1,C+1):\n",
    "        # prepare temporary C vs notC sub-probem labels\n",
    "        y_temp = copy.deepcopy(y)\n",
    "        ind = np.argwhere(y_temp.astype(int) == c)\n",
    "        ind = ind[:,0]\n",
    "        ind2 = np.argwhere(y_temp.astype(int) != c)\n",
    "        ind2 = ind2[:,0]\n",
    "        y_temp[ind] = 1\n",
    "        y_temp[ind2] = -1\n",
    "\n",
    "        # store best weight for final classification \n",
    "        cost = cost_lib.Setup(x,y_temp,cost_name).cost_func\n",
    "        weight_history,cost_history = optimizers.gradient_descent(cost,alpha_choice,max_its,w)\n",
    "\n",
    "        # store each weight history\n",
    "        weight_histories.append(weight_history)\n",
    "        \n",
    "    # combine each individual classifier weights into single weight \n",
    "    # matrix per step\n",
    "    R = len(weight_histories[0])\n",
    "    combined_weights = []\n",
    "    for r in range(R):\n",
    "        a = []\n",
    "        for c in range(C):\n",
    "            a.append(weight_histories[c][r])\n",
    "        a = np.array(a).T\n",
    "        a = a[0,:,:]\n",
    "        combined_weights.append(a)\n",
    "        \n",
    "    # run combined weight matrices through fusion rule to calculate\n",
    "    # number of misclassifications per step\n",
    "    counter = cost_lib.Setup(x,y,'fusion_rule').cost_func\n",
    "    count_history = [counter(v) for v in combined_weights]\n",
    "        \n",
    "    return combined_weights, count_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "data = np.loadtxt(datapath + 'singleinput_multiclass_dataset.csv',delimiter = ',')\n",
    "\n",
    "# get input/output pairs\n",
    "x = data[:,:-1:].T \n",
    "y = data[:,-1:] \n",
    "\n",
    "# create normalizer/inverse normalizer\n",
    "normalizer,inverse_normalizer = normalizers.standard_normalizer(x)\n",
    "\n",
    "# normalize input\n",
    "x_normalized = normalizer(x)\n",
    "\n",
    "# run one-versus-all\n",
    "combined_weights, count_history = one_versus_all(x_normalized,y,max_its = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_cost_histories() missing 2 required positional arguments: 'cost_histories' and 'count_histories'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-9e6eabbb8127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plot the cost function history for a given run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassification_plotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_cost_histories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount_history\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'standard'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'normalized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: plot_cost_histories() missing 2 required positional arguments: 'cost_histories' and 'count_histories'"
     ]
    }
   ],
   "source": [
    "# plot the cost function history for a given run\n",
    "classification_plotter.plot_cost_histories([count_history],start = 0,labels = ['standard','normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_history[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = cost_lib.Setup(x_normalized,y,'fusion_rule').cost_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter(combined_weights[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_weights[100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
